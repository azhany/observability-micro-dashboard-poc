# Risk Profile: Story 4.2 Stateful Alerting Engine

Date: 2026-01-18
Reviewer: Quinn (Test Architect)

## Executive Summary

- Total Risks Identified: 6
- Critical Risks: 0
- High Risks: 2
- Risk Score: 64/100 (Medium Risk)

The stateful alerting engine introduces significant technical complexity regarding time-based state transitions and database performance. The primary concerns involve the "Alert Storm" phenomenon and the overhead of querying raw metrics at high frequency.

## High Risks Requiring Attention

### 1. OPS-001: Alert Flapping / Storming

**Score: 6 (High)**
**Probability**: High - Without hysteresis or deadbands, metrics hovering near a threshold will trigger constant state changes.
**Impact**: Medium - Causes "alert fatigue" where operators ignore the system due to noise.
**Mitigation**:
- Implement a small "recovery margin" (hysteresis).
- Ensure the `PENDING` state is strictly tied to the `duration` parameter.
- Add rate limiting for alert history records if needed.
**Testing Focus**: Simulate "noisy" metrics fluctuating around the threshold to verify state stability.

### 2. PERF-004: Evaluator Query Overhead

**Score: 6 (High)**
**Probability**: Medium - Querying `metrics_raw` for every active rule every 30 seconds will scale poorly as tenant/rule counts grow.
**Impact**: High - Could slow down the entire database, affecting metric ingestion and dashboard responsiveness.
**Mitigation**:
- Optimize queries to use indexes on `(metric_name, tenant_id, created_at)`.
- Consider evaluating rules against the 1m rollup data for rules with longer durations (>1m).
- Batch evaluation logic to minimize DB roundtrips.
**Testing Focus**: Load test with 500+ active rules across multiple tenants.

## Risk Distribution

### By Category
- **Technical**: 2 risks
- **Performance**: 2 risks
- **Operational**: 1 risk
- **Security**: 1 risk

### By Component
- **Evaluator Job**: 4 risks
- **Database (Schema/Indexes)**: 2 risks

## Detailed Risk Register

| Risk ID  | Description | Probability | Impact | Score | Priority |
| :--- | :--- | :--- | :--- | :--- | :--- |
| OPS-001 | Alert Flapping / Noise | High (3) | Medium (2) | 6 | High |
| PERF-004 | Evaluator Query Overhead | Medium (2) | High (3) | 6 | High |
| TECH-003 | State Persistence Gaps | Medium (2) | Medium (2) | 4 | Medium |
| TECH-002 | Job Overlap (Evaluation > 30s) | Medium (2) | Medium (2) | 4 | Medium |
| SEC-001  | Tenant Data Leakage | Low (1) | High (3) | 3 | Low |
| PERF-005 | Alert History Table Growth | Low (1) | Medium (2) | 2 | Low |

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Tests
- **State Machine Integrity**: Extensive unit tests for `OK -> PENDING -> FIRING` transitions, specifically verifying that the `duration` timer resets if the threshold is no longer breached during the `PENDING` phase.
- **Performance Benchmarking**: Measure DB CPU/IO while the evaluator runs against a large `metrics_raw` table.

### Priority 2: High Risk Tests
- **Tenant Isolation**: Verify that a rule for Tenant A cannot be triggered by metrics from Tenant B, even if the metric names are identical.
- **Overlap Prevention**: Ensure Laravel's `withoutOverlapping()` is functional for the evaluator job.

### Priority 3: Medium/Low Risk Tests
- **Historical Accuracy**: Verify `alerts` table history correctly reflects the timeline of a real incident.

## Risk Acceptance Criteria

### Must Fix Before Production
- Indexed queries for the evaluator (PERF-004).
- Verified duration-based state machine logic (TECH-003).
- Multi-tenant data filtering in the evaluation query (SEC-001).

### Can Deploy with Mitigation
- Hysteresis/Flap detection (can be added as a subsequent enhancement if the initial volume is low).

## Monitoring Requirements
- **Evaluator Runtime**: Alert if evaluation takes > 25 seconds.
- **Alert Frequency**: Monitor the rate of state transitions per tenant.
- **Missing Evaluations**: Monitor if the evaluator job fails to run within its scheduled window.

## Risk Review Triggers
- Introduction of complex rule types (e.g., "Average of X over Y").
- Scaling beyond 1,000 active alert rules.
- Transitioning to a distributed evaluator architecture.
